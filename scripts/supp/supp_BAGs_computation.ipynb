{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65176697-73f6-4266-abef-1325ee5a290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Caitlin Rivers\n",
    "Analysis functions for package epipy.\n",
    "\"\"\"\n",
    "def _get_table_labels(table):\n",
    "    \"\"\"\n",
    "    Returns classic a, b, c, d labels for contingency table calcs.\n",
    "    \"\"\"\n",
    "    a = table[0][0]\n",
    "    b = table[0][1]\n",
    "    c = table[1][0]\n",
    "    d = table[1][1]\n",
    "\n",
    "    return a, b, c, d\n",
    "\n",
    "\n",
    "def _ordered_table(table):\n",
    "    \"\"\"\n",
    "    Determine type of table input. Find classic a, b, c, d labels\n",
    "    for contigency table calculations.\n",
    "    \"\"\"\n",
    "    if type(table) is list:\n",
    "        a, b, c, d = _get_table_labels(table)\n",
    "    elif type(table) is pd.core.frame.DataFrame:\n",
    "        a, b, c, d = _get_table_labels(table.values)\n",
    "    elif type(table) is np.ndarray:\n",
    "        a, b, c, d = _get_table_labels(table)\n",
    "    else:\n",
    "        raise TypeError('table format not recognized')\n",
    "\n",
    "    return a, b, c, d\n",
    "\n",
    "\n",
    "def _conf_interval(ratio, std_error):\n",
    "    \"\"\"\n",
    "    Calculate 95% confidence interval for odds ratio and relative risk.\n",
    "    \"\"\"\n",
    "\n",
    "    _lci = np.log(ratio) - 1.96*std_error\n",
    "    _uci = np.log(ratio) + 1.96*std_error\n",
    "\n",
    "    lci = round(np.exp(_lci), 2)\n",
    "    uci = round(np.exp(_uci), 2)\n",
    "\n",
    "    return (lci, uci)\n",
    "\n",
    "def _numeric_summary(column):\n",
    "    \"\"\"\n",
    "    Finds count, number of missing values, min, median, mean, std, and\n",
    "    max.\n",
    "    See summary()\n",
    "    \"\"\"\n",
    "    names = ['count', 'missing', 'min', 'median', 'mean', 'std', 'max']\n",
    "    _count = len(column)\n",
    "    _miss = _count - len(column.dropna())\n",
    "    _min = column.min()\n",
    "    _median = column.median()\n",
    "    _mean = column.mean()\n",
    "    _std = column.std()\n",
    "    _max = column.max()\n",
    "    summ = pd.Series([_count, _miss, _min, _median, _mean, _std, _max], index=names)\n",
    "\n",
    "    return summ\n",
    "\n",
    "\n",
    "def _categorical_summary(column, n=None):\n",
    "    \"\"\"\n",
    "    Finds count and frequency of each unique value in the column.\n",
    "    See summary().\n",
    "    \"\"\"\n",
    "    if n is not None:\n",
    "        _count = column.value_counts()[:n]\n",
    "    else:\n",
    "        _count = column.value_counts()\n",
    "    names = ['count', 'freq']\n",
    "    _freq = column.value_counts(normalize=True)[:n]\n",
    "    summ = pd.DataFrame([_count, _freq], index=names).T\n",
    "\n",
    "    return summ\n",
    "\n",
    "\n",
    "def _summary_calc(column, by=None):\n",
    "    \"\"\"\n",
    "    Calculates approporiate summary statistics based on data type.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    column = one column (series) of pandas df\n",
    "    by = optional. stratifies summary statistics by each value in the\n",
    "                column.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    if column data type is numeric, returns summary statistics\n",
    "    if column data type is an object, returns count and frequency of\n",
    "        top 5 most common values\n",
    "    \"\"\"\n",
    "    if column.dtype == 'float64' or column.dtype == 'int64':\n",
    "        coltype = 'numeric'\n",
    "    elif column.dtype == 'object':\n",
    "        coltype = 'object'\n",
    "\n",
    "\n",
    "    if by is None:\n",
    "        if coltype == 'numeric':\n",
    "            summ = _numeric_summary(column)\n",
    "\n",
    "        elif coltype == 'object':\n",
    "            summ = _categorical_summary(column, 5)\n",
    "\n",
    "    else:\n",
    "\n",
    "        if coltype == 'numeric':\n",
    "            column_list = []\n",
    "\n",
    "            vals = by.dropna().unique()\n",
    "            for value in vals:\n",
    "                subcol = column[by == value]\n",
    "                summcol = _numeric_summary(subcol)\n",
    "                column_list.append(summcol)\n",
    "\n",
    "            summ = pd.DataFrame(column_list, index=vals)\n",
    "\n",
    "        elif coltype == 'object':\n",
    "            subcol = column.groupby(by)\n",
    "            summ = _categorical_summary(subcol)\n",
    "            #summ = _summ.sort_values(by=subcol)\n",
    "\n",
    "    return summ\n",
    "\n",
    "\n",
    "def reproduction_number(G, index_cases=True, plot=True):\n",
    "    \"\"\"\n",
    "    Finds each case's basic reproduction number, which is the number of secondary\n",
    "    infections each case produces.\n",
    "    PARAMETERS\n",
    "    ----------------\n",
    "    G = networkx object\n",
    "    index_cases = include index nodes, i.e. those at generation 0. Default is True.\n",
    "                  Excluding them is useful if you want to calculate the human to human\n",
    "                  reproduction number without considering zoonotically acquired cases.\n",
    "    summary = print summary statistics of the case reproduction numbers\n",
    "    plot = create histogram of case reproduction number distribution.\n",
    "    RETURNS\n",
    "    ----------------\n",
    "    pandas series of case reproduction numbers and matplotlib figure\n",
    "    and axis objects if plot=True\n",
    "    \"\"\"\n",
    "\n",
    "    if index_cases == True:\n",
    "        R = pd.Series(G.out_degree())\n",
    "\n",
    "    elif index_cases == False:\n",
    "        degrees = {}\n",
    "\n",
    "        for n in G.node:\n",
    "            if G.node[n]['generation'] > 0:\n",
    "                degrees[n] = G.out_degree(n)\n",
    "        R = pd.Series(degrees)\n",
    "\n",
    "    print('Summary of reproduction numbers')\n",
    "    print(R.describe(), '\\n')\n",
    "\n",
    "    if plot == True:\n",
    "        fig, ax = plt.subplots()\n",
    "        R.hist(ax=ax, alpha=.5)\n",
    "        ax.set_xlabel('Secondary cases')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.grid(False)\n",
    "        return R, fig, ax\n",
    "\n",
    "    else:\n",
    "        return R\n",
    "\n",
    "\n",
    "def generation_analysis(G, attribute, plot=True):\n",
    "    \"\"\"\n",
    "    Analyzes an attribute, e.g. health status, by generation.\n",
    "    PARAMETERS\n",
    "    -------------\n",
    "    G = networkx object\n",
    "    attribute = case attribute for analysis, e.g. health status or sex\n",
    "    table = print cross table of attribute by generation. Default is true.\n",
    "    plot = produce histogram of attribute by generation. Default is true.\n",
    "    RETURNS\n",
    "    --------------\n",
    "    matplotlib figure and axis objects\n",
    "    \"\"\"\n",
    "\n",
    "    gen_df = pd.DataFrame(G.node).T\n",
    "\n",
    "    print('{} by generation').format(attribute)\n",
    "    table = pd.crosstab(gen_df.generation, gen_df[attribute], margins=True)\n",
    "    print(table, '\\n')\n",
    "\n",
    "    if plot == True:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_aspect('auto')\n",
    "        pd.crosstab(gen_df.generation, gen_df[attribute]).plot(kind='bar', ax=ax, alpha=.5, rot=0)\n",
    "        ax.set_xlabel('Generation')\n",
    "        ax.set_ylabel('Case count')\n",
    "        ax.grid(False)\n",
    "        ax.legend(loc='best');\n",
    "        return fig, ax, table\n",
    "    else:\n",
    "        return table\n",
    "\n",
    "\n",
    "def create_2x2(df, row, column, row_order, col_order):\n",
    "    \"\"\"\n",
    "    2x2 table of disease and exposure in traditional epi order.\n",
    "    Table format:\n",
    "                Disease\n",
    "    Exposure    YES     NO\n",
    "    YES         a       b\n",
    "    NO          c       d\n",
    "    PARAMETERS\n",
    "    -----------------------\n",
    "    df = pandas dataframe of line listing\n",
    "    row = name of exposure row as string\n",
    "    column = name of outcome column as string\n",
    "    row_order = list of length 2 of row values in yes/no order.\n",
    "                Example: ['Exposed', 'Unexposed']\n",
    "    col_order = list of length 2 column values in yes/no order.\n",
    "                Example: ['Sick', 'Not sick']\n",
    "    RETURNS\n",
    "    ------------------------\n",
    "    pandas dataframe of 2x2 table. Prints odds ratio and relative risk.\n",
    "    \"\"\"\n",
    "    if type(col_order) != list or type(row_order) != list:\n",
    "        raise TypeError('row_order and col_order must each be lists of length 2')\n",
    "\n",
    "    if len(col_order) != 2 or len(row_order) != 2:\n",
    "        raise AssertionError('row_order and col_order must each be lists of length 2')\n",
    "\n",
    "    _table = pd.crosstab(df[row], df[column], margins=True).to_dict()\n",
    "\n",
    "    trow = row_order[0]\n",
    "    brow = row_order[1]\n",
    "    tcol = col_order[0]\n",
    "    bcol = col_order[1]\n",
    "\n",
    "    table = pd.DataFrame(_table, index=[trow, brow, 'All'], columns=[tcol, bcol, 'All'])\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def analyze_2x2(table):\n",
    "    \"\"\"\n",
    "    Prints odds ratio, relative risk, and chi square.\n",
    "    See also create_2x2(), odds_ratio(), relative_risk(), and chi2()\n",
    "    PARAMETERS\n",
    "    --------------------\n",
    "    2x2 table as pandas dataframe, numpy array, or list in format [a, b, c, d]\n",
    "    Table format:\n",
    "                Disease\n",
    "    Exposure    YES     NO\n",
    "    YES         a       b\n",
    "    NO          c       d\n",
    "    \"\"\"\n",
    "\n",
    "    odds_ratio(table)\n",
    "    relative_risk(table)\n",
    "    attributable_risk(table)\n",
    "    chi2(table)\n",
    "\n",
    "\n",
    "def odds_ratio(table):\n",
    "    \"\"\"\n",
    "    Calculates the odds ratio and 95% confidence interval. See also\n",
    "    analyze_2x2()\n",
    "    *Cells in the table with a value of 0 will be replaced with .1\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe, numpy array, or list in [a, b, c, d] format.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns and prints odds ratio and tuple of 95% confidence interval\n",
    "    \"\"\"\n",
    "    \n",
    "    a, b, c, d = _ordered_table(table)\n",
    "\n",
    "    ratio = (a*d)/(b*c)\n",
    "    or_se = np.sqrt((1/a)+(1/b)+(1/c)+(1/d))\n",
    "    or_ci = _conf_interval(ratio, or_se)\n",
    "    print('Odds ratio: {} (95% CI: {})'.format(round(ratio, 2), or_ci))\n",
    "\n",
    "    return round(ratio, 2), or_ci\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def relative_risk(table, display=True):\n",
    "    \"\"\"\n",
    "    Calculates the relative risk and 95% confidence interval. See also\n",
    "    analyze_2x2().\n",
    "    *Cells in the table with a value of 0 will be replaced with .1\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe, numpy array, or list in [a, b, c, d] format.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns and prints relative risk and tuple of 95% confidence interval\n",
    "    \"\"\"\n",
    "    \n",
    "    a, b, c, d = _ordered_table(table)\n",
    "\n",
    "    rr = (a/(a+b))/(c/(c+d))\n",
    "    rr_se = np.sqrt(((1/a)+(1/c)) - ((1/(a+b)) + (1/(c+d))))\n",
    "    rr_ci = _conf_interval(rr, rr_se)\n",
    "\n",
    "    if display is not False:\n",
    "        print('Relative risk: {} (95% CI: {}-{})\\n'.format(round(rr, 2), round(rr_ci[0],2), round(rr_ci[1], 2)))\n",
    "\n",
    "    return rr, rr_ci\n",
    "\n",
    "\n",
    "def attributable_risk(table):\n",
    "    \"\"\"\n",
    "    Calculate the attributable risk, attributable risk percent,\n",
    "    and population attributable risk.\n",
    "    PARAMETERS\n",
    "    ----------------\n",
    "    table = 2x2 table. See 2x2_table()\n",
    "    RETURNS\n",
    "    ----------------\n",
    "    prints and returns attributable risk (AR), attributable risk percent\n",
    "    (ARP), population attributable risk (PAR) and population attributable\n",
    "    risk percent (PARP).\n",
    "    \"\"\"\n",
    "    a, b, c, d = _ordered_table(table)\n",
    "    N = a + b + c + d\n",
    "\n",
    "    ar = (a/(a+b))-(c/(c+d))\n",
    "    ar_se = np.sqrt(((a+c)/N)*(1-((a+c)/N))*((1/(a+b))+(1/(c+d))))\n",
    "    ar_ci = (round(ar-(1.96*ar_se), 2), round(ar+(1.96*ar_se), 2))\n",
    "\n",
    "    rr, rci = relative_risk(table, display=False)\n",
    "    arp = 100*((rr-1)/(rr))\n",
    "    arp_se = (1.96*ar_se)/ar\n",
    "    arp_ci = (round(arp-arp_se, 2), round(arp+arp_se, 3))\n",
    "\n",
    "    par = ((a+c)/N) - (c/(c+d))\n",
    "    parp = 100*(par/(((a+c)/N)))\n",
    "\n",
    "    print('Attributable risk: {} (95% CI: {})'.format(round(ar, 3), ar_ci))\n",
    "    print('Attributable risk percent: {}% (95% CI: {})'.format(round(arp, 2), arp_ci))\n",
    "    print('Population attributable risk: {}'.format(round(par, 3)))\n",
    "    print('Population attributable risk percent: {}% \\n'.format(round(parp, 2)))\n",
    "\n",
    "    return ar, arp, par, parp\n",
    "\n",
    "\n",
    "def my_attributable_risk(table, print_result =  True):\n",
    "    \"\"\"\n",
    "    Calculate the attributable risk, attributable risk percent,\n",
    "    and population attributable risk.\n",
    "    PARAMETERS\n",
    "    ----------------\n",
    "    table = 2x2 table. See 2x2_table()\n",
    "    RETURNS\n",
    "    ----------------\n",
    "    prints and returns attributable risk (AR), attributable risk percent\n",
    "    (ARP), population attributable risk (PAR) and population attributable\n",
    "    risk percent (PARP).\n",
    "    \"\"\"\n",
    "    a, b, c, d = _ordered_table(table)\n",
    "    N = a + b + c + d\n",
    "\n",
    "    ar = (a/(a+b))-(c/(c+d))\n",
    "    ar_se = np.sqrt(((a+c)/N)*(1-((a+c)/N))*((1/(a+b))+(1/(c+d))))\n",
    "    ar_ci = (round(ar-(1.96*ar_se), 2), round(ar+(1.96*ar_se), 2))\n",
    "\n",
    "    rr, rci = relative_risk(table, display=False)\n",
    "    arp = 100*((rr-1)/(rr))\n",
    "    arp_se = (1.96*ar_se)/ar\n",
    "    arp_ci = (round(arp-arp_se, 2), round(arp+arp_se, 3))\n",
    "\n",
    "    par = ((a+c)/N) - (c/(c+d))\n",
    "    parp = 100*(par/(((a+c)/N)))\n",
    "\n",
    "    if(print_result):\n",
    "        print('Attributable risk: {} (95% CI: {})'.format(round(ar, 3), ar_ci))\n",
    "        print('Attributable risk percent: {}% (95% CI: {})'.format(round(arp, 2), arp_ci))\n",
    "        print('Population attributable risk: {}'.format(round(par, 3)))\n",
    "        print('Population attributable risk percent: {}% \\n'.format(round(parp, 2)))\n",
    "\n",
    "    return arp, arp_ci\n",
    "\n",
    "def chi2(table):\n",
    "    \"\"\"\n",
    "    Scipy.stats function to calculate chi square.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe or numpy array. See also\n",
    "    analyze_2x2().\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns chi square with yates correction, p value,\n",
    "    degrees of freedom, and array of expected values.\n",
    "    prints chi square and p value\n",
    "    \"\"\"\n",
    "    chi2, p, dof, expected = chi2_contingency(table)\n",
    "    #print('Chi square: {}'.format(chi2))\n",
    "    #print('p value: {}'.format(p))\n",
    "\n",
    "    return chi2, p, dof, expected\n",
    "\n",
    "\n",
    "def summary(data, by=None):\n",
    "    \"\"\"\n",
    "    Displays approporiate summary statistics for each column in a line listing.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    data = pandas data frame or series\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    for each column in the dataframe, or for hte series:\n",
    "    - if column data type is numeric, returns summary statistics\n",
    "    - if column data type is non-numeric, returns count and frequency of\n",
    "        top 5 most common values.\n",
    "    EXAMPLE\n",
    "    ----------------------\n",
    "    df = pd.DataFrame({'Age' : [10, 12, 14], 'Group' : ['A', 'B', 'B'] })\n",
    "    In: summary(df.Age)\n",
    "    Out:\n",
    "        count       3\n",
    "        missing     0\n",
    "        min        10\n",
    "        median     12\n",
    "        mean       12\n",
    "        std         2\n",
    "        max        14\n",
    "        dtype: float64\n",
    "    In: summary(df.Group)\n",
    "    Out:\n",
    "           count      freq\n",
    "        B      2  0.666667\n",
    "        A      1  0.333333\n",
    "    In:summary(df.Age, by=df.Group)\n",
    "    Out     count  missing  min  median  mean      std  max\n",
    "        A      1        0   10      10    10       NaN   10\n",
    "        B      2        0   12      13    13  1.414214   14\n",
    "    \"\"\"\n",
    "    if type(data) == pd.core.series.Series:\n",
    "        summ = _summary_calc(data, by=by)\n",
    "        return summ\n",
    "\n",
    "    elif type(data) == pd.core.frame.DataFrame:\n",
    "        for column in data:\n",
    "            summ = _summary_calc(data[column], by=None)\n",
    "            print('----------------------------------')\n",
    "            print(column, '\\n')\n",
    "            print(summ)\n",
    "\n",
    "\n",
    "def diagnostic_accuracy(table, display=True):\n",
    "    \"\"\"\n",
    "    Calculates the sensitivity, specificity, negative and positive predictive values\n",
    "    of a 2x2 table with 95% confidence intervals. Note that confidence intervals\n",
    "    are made based on a normal approximation, and may not be appropriate for\n",
    "    small sample sizes.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe, numpy array, or list in [a, b, c, d] format.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns and prints diagnostic accuracy estimates and tuple of 95% confidence interval\n",
    "    Author: Eric Lofgren\n",
    "    \"\"\"\n",
    "    a, b, c, d = _ordered_table(table)\n",
    "\n",
    "    sen = (a/(a+c))\n",
    "    sen_se = np.sqrt((sen*(1-sen))/(a+c))\n",
    "    sen_ci = (sen-(1.96*sen_se),sen+(1.96*sen_se))\n",
    "    spec = (d/(b+d))\n",
    "    spec_se = np.sqrt((spec*(1-spec))/(b+d))\n",
    "    spec_ci = (spec-(1.96*spec_se),spec+(1.96*spec_se))\n",
    "    PPV = (a/(a+b))\n",
    "    PPV_se = np.sqrt((PPV*(1-PPV))/(a+b))\n",
    "    PPV_ci = (PPV-(1.96*PPV_se),PPV+(1.96*PPV_se))\n",
    "    NPV = (d/(c+d))\n",
    "    NPV_se = np.sqrt((NPV*(1-NPV))/(c+d))\n",
    "    NPV_ci = (NPV-(1.96*NPV_se),NPV+(1.96*NPV_se))\n",
    "\n",
    "    if display is not False:\n",
    "        print('Sensitivity: {} (95% CI: {})\\n'.format(round(sen, 2), sen_ci))\n",
    "        print('Specificity: {} (95% CI: {})\\n'.format(round(spec, 2), spec_ci))\n",
    "        print('Positive Predictive Value: {} (95% CI: {})\\n'.format(round(PPV, 2), PPV_ci))\n",
    "        print('Negative Predictive Value: {} (95% CI: {})\\n'.format(round(NPV, 2), NPV_ci))\n",
    "\n",
    "    return sen,sen_ci,spec,spec_ci,PPV,PPV_ci,NPV,NPV_ci\n",
    "\n",
    "\n",
    "def kappa_agreement(table, display=True):\n",
    "    \"\"\"\n",
    "    Calculated an unweighted Cohen's kappa statistic of observer agreement for a 2x2 table.\n",
    "    Note that the kappa statistic can be extended to an n x m table, but this\n",
    "    implementation is restricted to 2x2.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe, numpy array, or list in [a, b, c, d] format.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns and prints the Kappa statistic\n",
    "    Author: Eric Lofgren\n",
    "    \"\"\"\n",
    "    a, b, c, d = _ordered_table(table)\n",
    "    n = a + b + c + d\n",
    "    pr_a = ((a+d)/n)\n",
    "    pr_e = (((a+b)/n) * ((a+c)/n)) + (((c+d)/n) * ((b+d)/n))\n",
    "    k = (pr_a - pr_e)/(1 - pr_e)\n",
    "    if display is not False:\n",
    "        print(\"Cohen's Kappa: {}\\n\").format(round(k, 2))\n",
    "\n",
    "    return k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_directional_accuracy(y_true, y_pred):\n",
    "    \n",
    "    differences = np.array(y_pred) - np.array(y_true) \n",
    "    signs = np.sign(differences)\n",
    "    mde = np.mean(signs)\n",
    "    return mde\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    absolute_errors = np.abs(y_pred - y_true)\n",
    "    mae = np.mean(absolute_errors)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "def coef_pval(coef_array_mean_, X_, y_, y_p):\n",
    "\n",
    "    n = X_.shape[0]\n",
    "    t = coef_tval(coef_array_mean_, X_, y_, y_p)\n",
    "    p = 2 * (1 - scipy.stats.t.cdf(abs(t), n - 1))\n",
    "    return p\n",
    "\n",
    "\n",
    "def coef_tval(coef_array_mean_, X_, y_, y_p):\n",
    "    \n",
    "    '''\n",
    "        coef_tval for OLS of statsmodels\n",
    "    '''\n",
    "    \n",
    "    a = np.array(coef_array_mean_[0][0]/ coef_se(X_, y_, y_p)[0])\n",
    "    b = np.array(coef_array_mean_[1::].flatten() / coef_se(X_, y_, y_p)[1:])\n",
    "    return np.append(a, b)\n",
    "\n",
    "\n",
    "def coef_se(X_, y_, y_p):\n",
    "    \n",
    "    '''\n",
    "        coef_se for OLS of statsmodels\n",
    "    '''\n",
    "    n = X_.shape[0]\n",
    "    \n",
    "    X1 = np.hstack((np.ones((n, 1)), np.matrix(X_)))\n",
    "    se_matrix = scipy.linalg.sqrtm(\n",
    "        metrics.mean_squared_error(y_, y_p) *\n",
    "        np.linalg.inv(X1.T * X1)\n",
    "    )\n",
    "    return np.diagonal(se_matrix)\n",
    "\n",
    "def directional_accuracy(predicted_values, true_values):\n",
    "    predicted_values = np.array(predicted_values)\n",
    "    true_values = np.array(true_values)\n",
    "    \n",
    "    difference_direction = np.sign(predicted_values - true_values)\n",
    "    \n",
    "    correct_direction_count = np.sum(difference_direction == 1) + np.sum(difference_direction == -1)\n",
    "    \n",
    "    directional_accuracy_score = correct_direction_count / len(predicted_values)\n",
    "    \n",
    "    return directional_accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from statsmodels.api import GLM\n",
    "from statsmodels.api import families\n",
    "\n",
    "\n",
    "def Regression_GBR(X, y, min_, max_, n_splits, params_b = -1, shaps_comp = False):\n",
    "    \n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    scaler = MinMaxScaler((0.05, 0.95))\n",
    "    scaling_data = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(scaling_data, columns= X.columns, index = X.index)\n",
    "\n",
    "    for i in range(min_, max_):\n",
    "        y_labels = []\n",
    "        y_predicts = []\n",
    "\n",
    "        y_pred_ = []\n",
    "        y_test_ = []\n",
    "        r_squared_l = []\n",
    "        rmse_l = []\n",
    "        mse_l = []\n",
    "\n",
    "        results_labels_df = pd.DataFrame(columns=['y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'ID'])\n",
    "\n",
    "        r_squared_ = 0\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=i)\n",
    "\n",
    "        lista_vars = list(X)\n",
    "\n",
    "        coef_array = np.zeros([len(lista_vars)+1, n_splits])\n",
    "        coef_t_value = np.zeros([len(lista_vars)+1, n_splits])\n",
    "        coef_p_value = np.zeros([len(lista_vars)+1, n_splits])\n",
    "\n",
    "        iter_ = 0\n",
    "        for train_index, test_index in kf.split(X):\n",
    "\n",
    "                import warnings\n",
    "                warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "                X_train, X_test = X.iloc[train_index, :], X.iloc[test_index,:]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "                if(params_b == -1):\n",
    "                    model = GradientBoostingRegressor(random_state=42)\n",
    "                else:\n",
    "                    model = GradientBoostingRegressor(random_state=42, **params_b)\n",
    "                    \n",
    "\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "                coef_array[0,iter_] = np.nan\n",
    "                coef_array[1::,iter_] = np.array(model.feature_importances_)\n",
    "\n",
    "                predicted_values = model.predict(X_test)\n",
    "\n",
    "                mse = np.mean((y_test - predicted_values)**2)\n",
    "\n",
    "                rmse = np.sqrt(mse)\n",
    "\n",
    "                y_labels.extend(list(y_test))\n",
    "                y_predicts.extend(list(predicted_values))\n",
    "\n",
    "\n",
    "\n",
    "                y_pred_.extend(list(predicted_values))\n",
    "                y_test_.extend(y_test)\n",
    "                \n",
    "                 #- No Adjust gap------------------------\n",
    "                gap_test =  predicted_values - y_test\n",
    "\n",
    "\n",
    "                gap_train =  model.predict(X_train) - y_train\n",
    "\n",
    "                slope, intercept, _, _, _ = linregress(y_train, gap_train)\n",
    "\n",
    "                corrected_gap = gap_test - (slope * y_test + intercept)\n",
    "                  \n",
    "                r_squared_l.append(r2_score(y_test, model.predict(X_test)))\n",
    "\n",
    "                mse_l.append(np.round(mean_squared_error(y_test, model.predict(X_test)), 6))\n",
    "                rmse_l.append(np.round(math.sqrt(mean_squared_error(y_test, model.predict(X_test))), 6))\n",
    "\n",
    "\n",
    "                result = np.column_stack((y_test, model.predict(X_test), gap_test, corrected_gap))\n",
    "                temp_df = pd.DataFrame(result, columns=['y_labels', 'y_pred', 'GAP', 'GAP_corrected'])\n",
    "                temp_df['ID'] = X_test.index\n",
    "\n",
    "                results_labels_df = pd.concat([results_labels_df, temp_df], ignore_index=True)\n",
    "\n",
    "                iter_+=1\n",
    "\n",
    "        n = len(y_predicts)\n",
    "        p = X.shape[1]\n",
    "        r_squared = r2_score(y_labels, y_predicts)\n",
    "        \n",
    "        mde = mean_directional_accuracy(y_labels, y_predicts)\n",
    "        mae = mean_absolute_error(y_labels, y_predicts)\n",
    "        \n",
    "        k = X.shape[1] - 1\n",
    "        r_squared_adj = 1 - (1 - r_squared) * (n - 1) / (n - k - 1)\n",
    "\n",
    "        mse  = (np.round(mean_squared_error(y_labels, y_predicts), 6))\n",
    "        rmse = (np.round(math.sqrt(mean_squared_error(y_labels, y_predicts)), 6))\n",
    "\n",
    "        r_squared_ = r_squared_/n_splits\n",
    "        F = (r_squared / p) / ((1 - r_squared) / (n - p - 1))\n",
    "        p_value = np.round(scipy.stats.f.sf(F, n, (n - p - 1)), 15)\n",
    "\n",
    "        F2 =r_squared  / (1 - r_squared)\n",
    "\n",
    "        coef_array_mean = np.zeros([len(lista_vars)+1, 1])\n",
    "        coef_array_std = np.zeros([len(lista_vars)+1, 1])\n",
    "\n",
    "        for j in range(len(lista_vars)+1):\n",
    "            coef_array_mean[j] = coef_array[j,:].mean()\n",
    "            coef_array_std[j] = coef_array[j,:].std()\n",
    "\n",
    "        coef_df = pd.DataFrame(\n",
    "                                index= ['_intercept'] + lista_vars,\n",
    "                                columns=['Estimate mean', 'Estimate std','t value', 'p value'])\n",
    "\n",
    "        coef_df['Estimate mean'] = coef_array_mean\n",
    "        coef_df['Estimate std'] = coef_array_std\n",
    "        coef_df['t value'] = coef_tval(coef_array_mean, X, y_labels, y_predicts)\n",
    "        coef_df['p value'] = coef_pval(coef_array_mean, X, y_labels, y_predicts)\n",
    "\n",
    "        coef_df.loc['_intercept', 'R2'] = r_squared\n",
    "        coef_df.loc['_intercept', 'R2 adj'] = r_squared_adj\n",
    "        coef_df.loc['_intercept', 'R2 [+-]'] = 1*np.std(r_squared_l)\n",
    "        coef_df.loc['_intercept', 'F2'] = F2\n",
    "        coef_df.loc['_intercept', 'mse'] = mse\n",
    "        coef_df.loc['_intercept', 'mse [+-]']  = 1*np.std(mse_l)\n",
    "        coef_df.loc['_intercept', 'rmse'] = rmse\n",
    "        coef_df.loc['_intercept', 'rmse [+-]'] = 1*np.std(rmse_l)\n",
    "        coef_df.loc['_intercept', 'outcome var'] = np.var(y)\n",
    "        #\n",
    "        coef_df.loc['_intercept', 'F'] = F\n",
    "        coef_df.loc['_intercept', 'F-p_value'] = p_value\n",
    "        \n",
    "        coef_df.loc['_intercept', 'MDE'] = mde\n",
    "        coef_df.loc['_intercept', 'MAE'] = mae\n",
    "\n",
    "        r_squared = r2_score(y_labels, y_predicts)\n",
    "        \n",
    "        \n",
    "    results_labels_df['y_pred_corrected'] =  results_labels_df['y_labels'] + results_labels_df['GAP_corrected']\n",
    "    results_labels_df = results_labels_df[['ID','y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'y_pred_corrected']]\n",
    "    \n",
    "    if(shaps_comp):\n",
    "        if(params_b == -1):\n",
    "            model = GradientBoostingRegressor(random_state=42)\n",
    "        else:\n",
    "            model = GradientBoostingRegressor(random_state=42, **params_b)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        explainer = shap.Explainer(model, X)\n",
    "        #shap_values = explainer(X)\n",
    "            \n",
    "        return [coef_df, r_squared_, results_labels_df, explainer]\n",
    "    else:\n",
    "        return [coef_df, r_squared_, results_labels_df]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581769a1-129d-49ae-8fbb-fdb157c82a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55375f2f-50b2-454b-9c19-f9419e2ed47d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f982d0ef-a8cf-4480-bc73-3995c008244d",
   "metadata": {},
   "source": [
    "To access the raw data, please submit a request to retrieve the databases. Contact details and the download link can be found in the table ([https://github.com/euroladbrainlat/Biobehavioral-age-gaps](https://github.com/euroladbrainlat/Biobehavioral-age-gaps)).\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991aebd-c02d-48a7-9d33-0369636cec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ee6b6-b74f-4dc0-b9a0-82ddbf31c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_list = ['Sex_1F_2M',\n",
    "              'Education',\n",
    "              'Barthel',\n",
    "              'Diabetes_1Y_0N',\n",
    "              'Hypertension_1Y_0N',\n",
    "              'Heart_Disease_1Y_0N',\n",
    "              'Physical_activity_1Y_0N',\n",
    "              'Cognition',\n",
    "              'Weight_problems_v02_1Y_0N',\n",
    "              'Well_being_domain',\n",
    "              'Sleep_problems_1Y_0N',\n",
    "              'Audition_problems', \n",
    "              'Vision_problems', \n",
    "              'Alcohol_consumption_1Y_0N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df875afd-9134-423f-8ec5-ed4818d6ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# Definir el espacio de búsqueda\n",
    "param_space = {\n",
    "    'n_estimators': Integer(50, 100),        \n",
    "    'learning_rate': Real(0.01, 0.2, prior='log-uniform'),  \n",
    "    'max_depth': Integer(1, 3),               \n",
    "    'min_samples_split': Integer(2, 5),       \n",
    "    'min_samples_leaf': Integer(1, 5),        \n",
    "}\n",
    "\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=42, loss = 'squared_error')\n",
    "\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=3,              \n",
    "    scoring='neg_mean_squared_error',  \n",
    "    cv=10,                  \n",
    "    n_jobs=-1,             \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X1 = data[vars_list]\n",
    "y1 = data['Age']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best Parameters:\", bayes_search.best_params_)\n",
    "print(\"Best Score:\", -bayes_search.best_score_)\n",
    "\n",
    "\n",
    "best_model = bayes_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test Score (R^2):\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec0cb3-5bdb-4eb3-8a8c-ff94a6c1b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best = OrderedDict([('learning_rate', 0.12287608582119026),\n",
    "#                 ('max_depth', 3),\n",
    "#                 ('min_samples_leaf', 2),\n",
    "#                 ('min_samples_split', 5),\n",
    "#                 ('n_estimators', 93)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63a5338-357b-4c39-a7e4-09774401a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data[vars_list]\n",
    "y1 = data['Age']\n",
    "j=10\n",
    "\n",
    "[coef_df_, r_squared_, results_labels_df_] = Regression_GBR(X1, y1, j, j+1, 10, best_model = -1, shaps_comp = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f84c76b-a834-4b54-bdb7-a2181af3e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_labels_df_['GAP_corrected'] ## Contains the BAGs values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b912a-93f8-4b47-8846-ebf2ef81a7dd",
   "metadata": {},
   "source": [
    "## Older and younger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a82f25-2655-47ca-a201-7e4804110a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youngers_ana = results_GB.copy()\n",
    "country_counts_total = df_youngers_ana['country'].value_counts()\n",
    "df_youngers_ana = df_youngers_ana[df_youngers_ana.GAP_corrected<0]\n",
    "Q1 = df_youngers_ana['GAP_corrected'].quantile(0.5)\n",
    "subset_Q1 = df_youngers_ana[df_youngers_ana['GAP_corrected'] <= Q1]\n",
    "\n",
    "\n",
    "X1 = subset_Q1[vars_list]\n",
    "y1 = subset_Q1['Age']\n",
    "j=10\n",
    "\n",
    "[coef_df_q1, r_squared_q1, results_labels_df_q1] = Regression_GBR(X1, y1, j, j+1, 10, params_b = -1, shaps_comp = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4215b4-478c-467b-bc9b-f263f164f880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b69414-3a1f-4a6d-8661-3485f52e3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olders_ana = results_GB.copy()\n",
    "country_counts_total = df_olders_ana['country'].value_counts()\n",
    "df_olders_ana = df_olders_ana[df_olders_ana.GAP_corrected>0]\n",
    "Q3 = df_olders_ana['GAP_corrected'].quantile(0.75)\n",
    "subset_Q3 = df_olders_ana[df_olders_ana['GAP_corrected'] >= Q3]\n",
    "\n",
    "X1 = subset_Q3[vars_list]\n",
    "y1 = subset_Q3['Age']\n",
    "j=10\n",
    "\n",
    "[coef_df_q3, r_squared_q3, results_labels_df_q3] = Regression_GBR(X1, y1, j, j+1, 10, params_b = -1, shaps_comp = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065e02d-a5f2-4eab-9d5a-98f92b899cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
