{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe9f40e-4f1c-4a0e-9b62-b144ed5f5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Caitlin Rivers\n",
    "Analysis functions for package epipy.\n",
    "\"\"\"\n",
    "def _get_table_labels(table):\n",
    "    \"\"\"\n",
    "    Returns classic a, b, c, d labels for contingency table calcs.\n",
    "    \"\"\"\n",
    "    a = table[0][0]\n",
    "    b = table[0][1]\n",
    "    c = table[1][0]\n",
    "    d = table[1][1]\n",
    "\n",
    "    return a, b, c, d\n",
    "\n",
    "\n",
    "def _ordered_table(table):\n",
    "    \"\"\"\n",
    "    Determine type of table input. Find classic a, b, c, d labels\n",
    "    for contigency table calculations.\n",
    "    \"\"\"\n",
    "    if type(table) is list:\n",
    "        a, b, c, d = _get_table_labels(table)\n",
    "    elif type(table) is pd.core.frame.DataFrame:\n",
    "        a, b, c, d = _get_table_labels(table.values)\n",
    "    elif type(table) is np.ndarray:\n",
    "        a, b, c, d = _get_table_labels(table)\n",
    "    else:\n",
    "        raise TypeError('table format not recognized')\n",
    "\n",
    "    return a, b, c, d\n",
    "\n",
    "\n",
    "def _conf_interval(ratio, std_error):\n",
    "    \"\"\"\n",
    "    Calculate 95% confidence interval for odds ratio and relative risk.\n",
    "    \"\"\"\n",
    "\n",
    "    _lci = np.log(ratio) - 1.96*std_error\n",
    "    _uci = np.log(ratio) + 1.96*std_error\n",
    "\n",
    "    lci = round(np.exp(_lci), 2)\n",
    "    uci = round(np.exp(_uci), 2)\n",
    "\n",
    "    return (lci, uci)\n",
    "\n",
    "def _numeric_summary(column):\n",
    "    \"\"\"\n",
    "    Finds count, number of missing values, min, median, mean, std, and\n",
    "    max.\n",
    "    See summary()\n",
    "    \"\"\"\n",
    "    names = ['count', 'missing', 'min', 'median', 'mean', 'std', 'max']\n",
    "    _count = len(column)\n",
    "    _miss = _count - len(column.dropna())\n",
    "    _min = column.min()\n",
    "    _median = column.median()\n",
    "    _mean = column.mean()\n",
    "    _std = column.std()\n",
    "    _max = column.max()\n",
    "    summ = pd.Series([_count, _miss, _min, _median, _mean, _std, _max], index=names)\n",
    "\n",
    "    return summ\n",
    "\n",
    "\n",
    "def _categorical_summary(column, n=None):\n",
    "    \"\"\"\n",
    "    Finds count and frequency of each unique value in the column.\n",
    "    See summary().\n",
    "    \"\"\"\n",
    "    if n is not None:\n",
    "        _count = column.value_counts()[:n]\n",
    "    else:\n",
    "        _count = column.value_counts()\n",
    "    names = ['count', 'freq']\n",
    "    _freq = column.value_counts(normalize=True)[:n]\n",
    "    summ = pd.DataFrame([_count, _freq], index=names).T\n",
    "\n",
    "    return summ\n",
    "\n",
    "\n",
    "def _summary_calc(column, by=None):\n",
    "    \"\"\"\n",
    "    Calculates approporiate summary statistics based on data type.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    column = one column (series) of pandas df\n",
    "    by = optional. stratifies summary statistics by each value in the\n",
    "                column.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    if column data type is numeric, returns summary statistics\n",
    "    if column data type is an object, returns count and frequency of\n",
    "        top 5 most common values\n",
    "    \"\"\"\n",
    "    if column.dtype == 'float64' or column.dtype == 'int64':\n",
    "        coltype = 'numeric'\n",
    "    elif column.dtype == 'object':\n",
    "        coltype = 'object'\n",
    "\n",
    "\n",
    "    if by is None:\n",
    "        if coltype == 'numeric':\n",
    "            summ = _numeric_summary(column)\n",
    "\n",
    "        elif coltype == 'object':\n",
    "            summ = _categorical_summary(column, 5)\n",
    "\n",
    "    else:\n",
    "\n",
    "        if coltype == 'numeric':\n",
    "            column_list = []\n",
    "\n",
    "            vals = by.dropna().unique()\n",
    "            for value in vals:\n",
    "                subcol = column[by == value]\n",
    "                summcol = _numeric_summary(subcol)\n",
    "                column_list.append(summcol)\n",
    "\n",
    "            summ = pd.DataFrame(column_list, index=vals)\n",
    "\n",
    "        elif coltype == 'object':\n",
    "            subcol = column.groupby(by)\n",
    "            summ = _categorical_summary(subcol)\n",
    "            #summ = _summ.sort_values(by=subcol)\n",
    "\n",
    "    return summ\n",
    "\n",
    "\n",
    "def reproduction_number(G, index_cases=True, plot=True):\n",
    "    \"\"\"\n",
    "    Finds each case's basic reproduction number, which is the number of secondary\n",
    "    infections each case produces.\n",
    "    PARAMETERS\n",
    "    ----------------\n",
    "    G = networkx object\n",
    "    index_cases = include index nodes, i.e. those at generation 0. Default is True.\n",
    "                  Excluding them is useful if you want to calculate the human to human\n",
    "                  reproduction number without considering zoonotically acquired cases.\n",
    "    summary = print summary statistics of the case reproduction numbers\n",
    "    plot = create histogram of case reproduction number distribution.\n",
    "    RETURNS\n",
    "    ----------------\n",
    "    pandas series of case reproduction numbers and matplotlib figure\n",
    "    and axis objects if plot=True\n",
    "    \"\"\"\n",
    "\n",
    "    if index_cases == True:\n",
    "        R = pd.Series(G.out_degree())\n",
    "\n",
    "    elif index_cases == False:\n",
    "        degrees = {}\n",
    "\n",
    "        for n in G.node:\n",
    "            if G.node[n]['generation'] > 0:\n",
    "                degrees[n] = G.out_degree(n)\n",
    "        R = pd.Series(degrees)\n",
    "\n",
    "    print('Summary of reproduction numbers')\n",
    "    print(R.describe(), '\\n')\n",
    "\n",
    "    if plot == True:\n",
    "        fig, ax = plt.subplots()\n",
    "        R.hist(ax=ax, alpha=.5)\n",
    "        ax.set_xlabel('Secondary cases')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.grid(False)\n",
    "        return R, fig, ax\n",
    "\n",
    "    else:\n",
    "        return R\n",
    "\n",
    "\n",
    "def generation_analysis(G, attribute, plot=True):\n",
    "    \"\"\"\n",
    "    Analyzes an attribute, e.g. health status, by generation.\n",
    "    PARAMETERS\n",
    "    -------------\n",
    "    G = networkx object\n",
    "    attribute = case attribute for analysis, e.g. health status or sex\n",
    "    table = print cross table of attribute by generation. Default is true.\n",
    "    plot = produce histogram of attribute by generation. Default is true.\n",
    "    RETURNS\n",
    "    --------------\n",
    "    matplotlib figure and axis objects\n",
    "    \"\"\"\n",
    "\n",
    "    gen_df = pd.DataFrame(G.node).T\n",
    "\n",
    "    print('{} by generation').format(attribute)\n",
    "    table = pd.crosstab(gen_df.generation, gen_df[attribute], margins=True)\n",
    "    print(table, '\\n')\n",
    "\n",
    "    if plot == True:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_aspect('auto')\n",
    "        pd.crosstab(gen_df.generation, gen_df[attribute]).plot(kind='bar', ax=ax, alpha=.5, rot=0)\n",
    "        ax.set_xlabel('Generation')\n",
    "        ax.set_ylabel('Case count')\n",
    "        ax.grid(False)\n",
    "        ax.legend(loc='best');\n",
    "        return fig, ax, table\n",
    "    else:\n",
    "        return table\n",
    "\n",
    "\n",
    "def create_2x2(df, row, column, row_order, col_order):\n",
    "    \"\"\"\n",
    "    2x2 table of disease and exposure in traditional epi order.\n",
    "    Table format:\n",
    "                Disease\n",
    "    Exposure    YES     NO\n",
    "    YES         a       b\n",
    "    NO          c       d\n",
    "    PARAMETERS\n",
    "    -----------------------\n",
    "    df = pandas dataframe of line listing\n",
    "    row = name of exposure row as string\n",
    "    column = name of outcome column as string\n",
    "    row_order = list of length 2 of row values in yes/no order.\n",
    "                Example: ['Exposed', 'Unexposed']\n",
    "    col_order = list of length 2 column values in yes/no order.\n",
    "                Example: ['Sick', 'Not sick']\n",
    "    RETURNS\n",
    "    ------------------------\n",
    "    pandas dataframe of 2x2 table. Prints odds ratio and relative risk.\n",
    "    \"\"\"\n",
    "    if type(col_order) != list or type(row_order) != list:\n",
    "        raise TypeError('row_order and col_order must each be lists of length 2')\n",
    "\n",
    "    if len(col_order) != 2 or len(row_order) != 2:\n",
    "        raise AssertionError('row_order and col_order must each be lists of length 2')\n",
    "\n",
    "    _table = pd.crosstab(df[row], df[column], margins=True).to_dict()\n",
    "\n",
    "    trow = row_order[0]\n",
    "    brow = row_order[1]\n",
    "    tcol = col_order[0]\n",
    "    bcol = col_order[1]\n",
    "\n",
    "    table = pd.DataFrame(_table, index=[trow, brow, 'All'], columns=[tcol, bcol, 'All'])\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def analyze_2x2(table):\n",
    "    \"\"\"\n",
    "    Prints odds ratio, relative risk, and chi square.\n",
    "    See also create_2x2(), odds_ratio(), relative_risk(), and chi2()\n",
    "    PARAMETERS\n",
    "    --------------------\n",
    "    2x2 table as pandas dataframe, numpy array, or list in format [a, b, c, d]\n",
    "    Table format:\n",
    "                Disease\n",
    "    Exposure    YES     NO\n",
    "    YES         a       b\n",
    "    NO          c       d\n",
    "    \"\"\"\n",
    "\n",
    "    odds_ratio(table)\n",
    "    relative_risk(table)\n",
    "    attributable_risk(table)\n",
    "    chi2(table)\n",
    "\n",
    "\n",
    "def odds_ratio(table):\n",
    "    \"\"\"\n",
    "    Calculates the odds ratio and 95% confidence interval. See also\n",
    "    analyze_2x2()\n",
    "    *Cells in the table with a value of 0 will be replaced with .1\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe, numpy array, or list in [a, b, c, d] format.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns and prints odds ratio and tuple of 95% confidence interval\n",
    "    \"\"\"\n",
    "    \n",
    "    a, b, c, d = _ordered_table(table)\n",
    "\n",
    "    ratio = (a*d)/(b*c)\n",
    "    or_se = np.sqrt((1/a)+(1/b)+(1/c)+(1/d))\n",
    "    or_ci = _conf_interval(ratio, or_se)\n",
    "    print('Odds ratio: {} (95% CI: {})'.format(round(ratio, 2), or_ci))\n",
    "\n",
    "    return round(ratio, 2), or_ci\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def relative_risk(table, display=True):\n",
    "    \"\"\"\n",
    "    Calculates the relative risk and 95% confidence interval. See also\n",
    "    analyze_2x2().\n",
    "    *Cells in the table with a value of 0 will be replaced with .1\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe, numpy array, or list in [a, b, c, d] format.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns and prints relative risk and tuple of 95% confidence interval\n",
    "    \"\"\"\n",
    "    \n",
    "    a, b, c, d = _ordered_table(table)\n",
    "\n",
    "    rr = (a/(a+b))/(c/(c+d))\n",
    "    rr_se = np.sqrt(((1/a)+(1/c)) - ((1/(a+b)) + (1/(c+d))))\n",
    "    rr_ci = _conf_interval(rr, rr_se)\n",
    "\n",
    "    if display is not False:\n",
    "        print('Relative risk: {} (95% CI: {}-{})\\n'.format(round(rr, 2), round(rr_ci[0],2), round(rr_ci[1], 2)))\n",
    "\n",
    "    return rr, rr_ci, rr_se\n",
    "\n",
    "\n",
    "def attributable_risk(table):\n",
    "    \"\"\"\n",
    "    Calculate the attributable risk, attributable risk percent,\n",
    "    and population attributable risk.\n",
    "    PARAMETERS\n",
    "    ----------------\n",
    "    table = 2x2 table. See 2x2_table()\n",
    "    RETURNS\n",
    "    ----------------\n",
    "    prints and returns attributable risk (AR), attributable risk percent\n",
    "    (ARP), population attributable risk (PAR) and population attributable\n",
    "    risk percent (PARP).\n",
    "    \"\"\"\n",
    "    a, b, c, d = _ordered_table(table)\n",
    "    N = a + b + c + d\n",
    "\n",
    "    ar = (a/(a+b))-(c/(c+d))\n",
    "    ar_se = np.sqrt(((a+c)/N)*(1-((a+c)/N))*((1/(a+b))+(1/(c+d))))\n",
    "    ar_ci = (round(ar-(1.96*ar_se), 2), round(ar+(1.96*ar_se), 2))\n",
    "\n",
    "    rr, rci = relative_risk(table, display=False)\n",
    "    arp = 100*((rr-1)/(rr))\n",
    "    arp_se = (1.96*ar_se)/ar\n",
    "    arp_ci = (round(arp-arp_se, 2), round(arp+arp_se, 3))\n",
    "\n",
    "    par = ((a+c)/N) - (c/(c+d))\n",
    "    parp = 100*(par/(((a+c)/N)))\n",
    "\n",
    "    print('Attributable risk: {} (95% CI: {})'.format(round(ar, 3), ar_ci))\n",
    "    print('Attributable risk percent: {}% (95% CI: {})'.format(round(arp, 2), arp_ci))\n",
    "    print('Population attributable risk: {}'.format(round(par, 3)))\n",
    "    print('Population attributable risk percent: {}% \\n'.format(round(parp, 2)))\n",
    "\n",
    "    return ar, arp, par, parp\n",
    "\n",
    "\n",
    "def my_attributable_risk(table, print_result =  True):\n",
    "    \"\"\"\n",
    "    Calculate the attributable risk, attributable risk percent,\n",
    "    and population attributable risk.\n",
    "    PARAMETERS\n",
    "    ----------------\n",
    "    table = 2x2 table. See 2x2_table()\n",
    "    RETURNS\n",
    "    ----------------\n",
    "    prints and returns attributable risk (AR), attributable risk percent\n",
    "    (ARP), population attributable risk (PAR) and population attributable\n",
    "    risk percent (PARP).\n",
    "    \"\"\"\n",
    "    a, b, c, d = _ordered_table(table)\n",
    "    N = a + b + c + d\n",
    "\n",
    "    ar = (a/(a+b))-(c/(c+d))\n",
    "    ar_se = np.sqrt(((a+c)/N)*(1-((a+c)/N))*((1/(a+b))+(1/(c+d))))\n",
    "    ar_ci = (round(ar-(1.96*ar_se), 2), round(ar+(1.96*ar_se), 2))\n",
    "\n",
    "    rr, rci, rr_se = relative_risk(table, display=False)\n",
    "    arp = 100*((rr-1)/(rr))\n",
    "    arp_se = (1.96*ar_se)/ar\n",
    "    arp_ci = (round(arp-arp_se, 2), round(arp+arp_se, 3))\n",
    "\n",
    "    par = ((a+c)/N) - (c/(c+d))\n",
    "    parp = 100*(par/(((a+c)/N)))\n",
    "\n",
    "    if(print_result):\n",
    "        print('Attributable risk: {} (95% CI: {})'.format(round(ar, 3), ar_ci))\n",
    "        print('Attributable risk percent: {}% (95% CI: {})'.format(round(arp, 2), arp_ci))\n",
    "        print('Population attributable risk: {}'.format(round(par, 3)))\n",
    "        print('Population attributable risk percent: {}% \\n'.format(round(parp, 2)))\n",
    "\n",
    "    return arp, arp_ci, rr, rci, rr_se\n",
    "\n",
    "def chi2(table):\n",
    "    \"\"\"\n",
    "    Scipy.stats function to calculate chi square.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe or numpy array. See also\n",
    "    analyze_2x2().\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns chi square with yates correction, p value,\n",
    "    degrees of freedom, and array of expected values.\n",
    "    prints chi square and p value\n",
    "    \"\"\"\n",
    "    chi2, p, dof, expected = chi2_contingency(table)\n",
    "    #print('Chi square: {}'.format(chi2))\n",
    "    #print('p value: {}'.format(p))\n",
    "\n",
    "    return chi2, p, dof, expected\n",
    "\n",
    "\n",
    "def summary(data, by=None):\n",
    "    \"\"\"\n",
    "    Displays approporiate summary statistics for each column in a line listing.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    data = pandas data frame or series\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    for each column in the dataframe, or for hte series:\n",
    "    - if column data type is numeric, returns summary statistics\n",
    "    - if column data type is non-numeric, returns count and frequency of\n",
    "        top 5 most common values.\n",
    "    EXAMPLE\n",
    "    ----------------------\n",
    "    df = pd.DataFrame({'Age' : [10, 12, 14], 'Group' : ['A', 'B', 'B'] })\n",
    "    In: summary(df.Age)\n",
    "    Out:\n",
    "        count       3\n",
    "        missing     0\n",
    "        min        10\n",
    "        median     12\n",
    "        mean       12\n",
    "        std         2\n",
    "        max        14\n",
    "        dtype: float64\n",
    "    In: summary(df.Group)\n",
    "    Out:\n",
    "           count      freq\n",
    "        B      2  0.666667\n",
    "        A      1  0.333333\n",
    "    In:summary(df.Age, by=df.Group)\n",
    "    Out     count  missing  min  median  mean      std  max\n",
    "        A      1        0   10      10    10       NaN   10\n",
    "        B      2        0   12      13    13  1.414214   14\n",
    "    \"\"\"\n",
    "    if type(data) == pd.core.series.Series:\n",
    "        summ = _summary_calc(data, by=by)\n",
    "        return summ\n",
    "\n",
    "    elif type(data) == pd.core.frame.DataFrame:\n",
    "        for column in data:\n",
    "            summ = _summary_calc(data[column], by=None)\n",
    "            print('----------------------------------')\n",
    "            print(column, '\\n')\n",
    "            print(summ)\n",
    "\n",
    "\n",
    "def diagnostic_accuracy(table, display=True):\n",
    "    \"\"\"\n",
    "    Calculates the sensitivity, specificity, negative and positive predictive values\n",
    "    of a 2x2 table with 95% confidence intervals. Note that confidence intervals\n",
    "    are made based on a normal approximation, and may not be appropriate for\n",
    "    small sample sizes.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe, numpy array, or list in [a, b, c, d] format.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns and prints diagnostic accuracy estimates and tuple of 95% confidence interval\n",
    "    Author: Eric Lofgren\n",
    "    \"\"\"\n",
    "    a, b, c, d = _ordered_table(table)\n",
    "\n",
    "    sen = (a/(a+c))\n",
    "    sen_se = np.sqrt((sen*(1-sen))/(a+c))\n",
    "    sen_ci = (sen-(1.96*sen_se),sen+(1.96*sen_se))\n",
    "    spec = (d/(b+d))\n",
    "    spec_se = np.sqrt((spec*(1-spec))/(b+d))\n",
    "    spec_ci = (spec-(1.96*spec_se),spec+(1.96*spec_se))\n",
    "    PPV = (a/(a+b))\n",
    "    PPV_se = np.sqrt((PPV*(1-PPV))/(a+b))\n",
    "    PPV_ci = (PPV-(1.96*PPV_se),PPV+(1.96*PPV_se))\n",
    "    NPV = (d/(c+d))\n",
    "    NPV_se = np.sqrt((NPV*(1-NPV))/(c+d))\n",
    "    NPV_ci = (NPV-(1.96*NPV_se),NPV+(1.96*NPV_se))\n",
    "\n",
    "    if display is not False:\n",
    "        print('Sensitivity: {} (95% CI: {})\\n'.format(round(sen, 2), sen_ci))\n",
    "        print('Specificity: {} (95% CI: {})\\n'.format(round(spec, 2), spec_ci))\n",
    "        print('Positive Predictive Value: {} (95% CI: {})\\n'.format(round(PPV, 2), PPV_ci))\n",
    "        print('Negative Predictive Value: {} (95% CI: {})\\n'.format(round(NPV, 2), NPV_ci))\n",
    "\n",
    "    return sen,sen_ci,spec,spec_ci,PPV,PPV_ci,NPV,NPV_ci\n",
    "\n",
    "\n",
    "def kappa_agreement(table, display=True):\n",
    "    \"\"\"\n",
    "    Calculated an unweighted Cohen's kappa statistic of observer agreement for a 2x2 table.\n",
    "    Note that the kappa statistic can be extended to an n x m table, but this\n",
    "    implementation is restricted to 2x2.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe, numpy array, or list in [a, b, c, d] format.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns and prints the Kappa statistic\n",
    "    Author: Eric Lofgren\n",
    "    \"\"\"\n",
    "    a, b, c, d = _ordered_table(table)\n",
    "    n = a + b + c + d\n",
    "    pr_a = ((a+d)/n)\n",
    "    pr_e = (((a+b)/n) * ((a+c)/n)) + (((c+d)/n) * ((b+d)/n))\n",
    "    k = (pr_a - pr_e)/(1 - pr_e)\n",
    "    if display is not False:\n",
    "        print(\"Cohen's Kappa: {}\\n\").format(round(k, 2))\n",
    "\n",
    "    return k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_directional_accuracy(y_true, y_pred):\n",
    "    \n",
    "    differences = np.array(y_pred) - np.array(y_true) \n",
    "    signs = np.sign(differences)\n",
    "    mde = np.mean(signs)\n",
    "    return mde\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    absolute_errors = np.abs(y_pred - y_true)\n",
    "    mae = np.mean(absolute_errors)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "def coef_pval(coef_array_mean_, X_, y_, y_p):\n",
    "\n",
    "    n = X_.shape[0]\n",
    "    t = coef_tval(coef_array_mean_, X_, y_, y_p)\n",
    "    p = 2 * (1 - scipy.stats.t.cdf(abs(t), n - 1))\n",
    "    return p\n",
    "\n",
    "\n",
    "def coef_tval(coef_array_mean_, X_, y_, y_p):\n",
    "    \n",
    "    '''\n",
    "        coef_tval for OLS of statsmodels\n",
    "    '''\n",
    "    \n",
    "    a = np.array(coef_array_mean_[0][0]/ coef_se(X_, y_, y_p)[0])\n",
    "    b = np.array(coef_array_mean_[1::].flatten() / coef_se(X_, y_, y_p)[1:])\n",
    "    return np.append(a, b)\n",
    "\n",
    "\n",
    "def coef_se(X_, y_, y_p):\n",
    "    \n",
    "    '''\n",
    "        coef_se for OLS of statsmodels\n",
    "    '''\n",
    "    n = X_.shape[0]\n",
    "    \n",
    "    X1 = np.hstack((np.ones((n, 1)), np.matrix(X_)))\n",
    "    se_matrix = scipy.linalg.sqrtm(\n",
    "        metrics.mean_squared_error(y_, y_p) *\n",
    "        np.linalg.inv(X1.T * X1)\n",
    "    )\n",
    "    return np.diagonal(se_matrix)\n",
    "\n",
    "def directional_accuracy(predicted_values, true_values):\n",
    "    predicted_values = np.array(predicted_values)\n",
    "    true_values = np.array(true_values)\n",
    "    \n",
    "    difference_direction = np.sign(predicted_values - true_values)\n",
    "    \n",
    "    correct_direction_count = np.sum(difference_direction == 1) + np.sum(difference_direction == -1)\n",
    "    \n",
    "    # Calcular la proporción de predicciones con dirección correcta\n",
    "    directional_accuracy_score = correct_direction_count / len(predicted_values)\n",
    "    \n",
    "    return directional_accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from statsmodels.api import GLM\n",
    "from statsmodels.api import families\n",
    "\n",
    "\n",
    "def Regression_GBR(X, y, min_, max_, n_splits, params_b = -1, shaps_comp = False):\n",
    "    \n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    scaler = MinMaxScaler((0.05, 0.95))\n",
    "    scaling_data = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(scaling_data, columns= X.columns, index = X.index)\n",
    "\n",
    "    for i in range(min_, max_):\n",
    "        y_labels = []\n",
    "        y_predicts = []\n",
    "        #n_splits = 7\n",
    "\n",
    "        y_pred_ = []\n",
    "        y_test_ = []\n",
    "        r_squared_l = []\n",
    "        rmse_l = []\n",
    "        mse_l = []\n",
    "\n",
    "        results_labels_df = pd.DataFrame(columns=['y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'ID'])\n",
    "\n",
    "        r_squared_ = 0\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=i)\n",
    "\n",
    "        lista_vars = list(X)\n",
    "\n",
    "        coef_array = np.zeros([len(lista_vars)+1, n_splits])\n",
    "        coef_t_value = np.zeros([len(lista_vars)+1, n_splits])\n",
    "        coef_p_value = np.zeros([len(lista_vars)+1, n_splits])\n",
    "\n",
    "        iter_ = 0\n",
    "        for train_index, test_index in kf.split(X):\n",
    "\n",
    "                import warnings\n",
    "                warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "                X_train, X_test = X.iloc[train_index, :], X.iloc[test_index,:]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "                if(params_b == -1):\n",
    "                    model = GradientBoostingRegressor(random_state=42)\n",
    "                else:\n",
    "                    model = GradientBoostingRegressor(random_state=42, **params_b)\n",
    "                    \n",
    "\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                coef_array[0,iter_] = np.nan\n",
    "                coef_array[1::,iter_] = np.array(model.feature_importances_)\n",
    "\n",
    "                predicted_values = model.predict(X_test)\n",
    "\n",
    "\n",
    "                mse = np.mean((y_test - predicted_values)**2)\n",
    "\n",
    "                rmse = np.sqrt(mse)\n",
    "\n",
    "                y_labels.extend(list(y_test))\n",
    "                y_predicts.extend(list(predicted_values))\n",
    "\n",
    "\n",
    "\n",
    "                y_pred_.extend(list(predicted_values))\n",
    "                y_test_.extend(y_test)\n",
    "                \n",
    "                gap_test =  predicted_values - y_test\n",
    "\n",
    "                gap_train =  model.predict(X_train) - y_train\n",
    "\n",
    "                slope, intercept, _, _, _ = linregress(y_train, gap_train)\n",
    "\n",
    "                corrected_gap = gap_test - (slope * y_test + intercept)\n",
    "\n",
    "\n",
    "                r_squared_l.append(r2_score(y_test, model.predict(X_test)))\n",
    "\n",
    "                mse_l.append(np.round(mean_squared_error(y_test, model.predict(X_test)), 6))\n",
    "                rmse_l.append(np.round(math.sqrt(mean_squared_error(y_test, model.predict(X_test))), 6))\n",
    "\n",
    "\n",
    "                result = np.column_stack((y_test, model.predict(X_test), gap_test, corrected_gap))\n",
    "                temp_df = pd.DataFrame(result, columns=['y_labels', 'y_pred', 'GAP', 'GAP_corrected'])\n",
    "                temp_df['ID'] = X_test.index\n",
    "\n",
    "                results_labels_df = pd.concat([results_labels_df, temp_df], ignore_index=True)\n",
    "\n",
    "                iter_+=1\n",
    "\n",
    "        n = len(y_predicts)\n",
    "        p = X.shape[1]\n",
    "        r_squared = r2_score(y_labels, y_predicts)\n",
    "        \n",
    "        mde = mean_directional_accuracy(y_labels, y_predicts)\n",
    "        mae = mean_absolute_error(y_labels, y_predicts)\n",
    "        \n",
    "        k = X.shape[1] - 1\n",
    "        r_squared_adj = 1 - (1 - r_squared) * (n - 1) / (n - k - 1)\n",
    "\n",
    "        mse  = (np.round(mean_squared_error(y_labels, y_predicts), 6))\n",
    "        rmse = (np.round(math.sqrt(mean_squared_error(y_labels, y_predicts)), 6))\n",
    "\n",
    "        r_squared_ = r_squared_/n_splits\n",
    "        F = (r_squared / p) / ((1 - r_squared) / (n - p - 1))\n",
    "        p_value = np.round(scipy.stats.f.sf(F, n, (n - p - 1)), 15)\n",
    "\n",
    "        F2 =r_squared  / (1 - r_squared)\n",
    "\n",
    "        coef_array_mean = np.zeros([len(lista_vars)+1, 1])\n",
    "        coef_array_std = np.zeros([len(lista_vars)+1, 1])\n",
    "\n",
    "        for j in range(len(lista_vars)+1):\n",
    "            coef_array_mean[j] = coef_array[j,:].mean()\n",
    "            coef_array_std[j] = coef_array[j,:].std()\n",
    "\n",
    "        coef_df = pd.DataFrame(\n",
    "                                index= ['_intercept'] + lista_vars,\n",
    "                                columns=['Estimate mean', 'Estimate std','t value', 'p value'])\n",
    "\n",
    "        coef_df['Estimate mean'] = coef_array_mean\n",
    "        coef_df['Estimate std'] = coef_array_std\n",
    "        coef_df['t value'] = coef_tval(coef_array_mean, X, y_labels, y_predicts)\n",
    "        coef_df['p value'] = coef_pval(coef_array_mean, X, y_labels, y_predicts)\n",
    "\n",
    "        coef_df.loc['_intercept', 'R2'] = r_squared\n",
    "        coef_df.loc['_intercept', 'R2 adj'] = r_squared_adj\n",
    "        coef_df.loc['_intercept', 'R2 [+-]'] = 1*np.std(r_squared_l)\n",
    "        coef_df.loc['_intercept', 'F2'] = F2\n",
    "        coef_df.loc['_intercept', 'mse'] = mse\n",
    "        coef_df.loc['_intercept', 'mse [+-]']  = 1*np.std(mse_l)\n",
    "        coef_df.loc['_intercept', 'rmse'] = rmse\n",
    "        coef_df.loc['_intercept', 'rmse [+-]'] = 1*np.std(rmse_l)\n",
    "        coef_df.loc['_intercept', 'outcome var'] = np.var(y)\n",
    "        #\n",
    "        coef_df.loc['_intercept', 'F'] = F\n",
    "        coef_df.loc['_intercept', 'F-p_value'] = p_value\n",
    "        \n",
    "        coef_df.loc['_intercept', 'MDE'] = mde\n",
    "        coef_df.loc['_intercept', 'MAE'] = mae\n",
    "\n",
    "        r_squared = r2_score(y_labels, y_predicts)\n",
    "\n",
    "        \n",
    "        \n",
    "    results_labels_df['y_pred_corrected'] =  results_labels_df['y_labels'] + results_labels_df['GAP_corrected']\n",
    "    results_labels_df = results_labels_df[['ID','y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'y_pred_corrected']]\n",
    "    \n",
    "    if(shaps_comp):\n",
    "        if(params_b == -1):\n",
    "            model = GradientBoostingRegressor(random_state=42)\n",
    "        else:\n",
    "            model = GradientBoostingRegressor(random_state=42, **params_b)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        explainer = shap.Explainer(model, X)\n",
    "            \n",
    "        return [coef_df, r_squared_, results_labels_df, explainer]\n",
    "    else:\n",
    "        return [coef_df, r_squared_, results_labels_df]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f3a968-93de-4524-9841-c1dded619830",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAGs_data_longitudinal = pd.read_excel('data/BAGs_data_longitudinal.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f806f8-34f0-44a0-acf5-7df164086c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b8b1a0f-60d3-4a0b-a3a7-77e453854fce",
   "metadata": {},
   "source": [
    "## Epidemiological metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b23e7-5762-481b-8f60-219873f19051",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = BAGs_data_longitudinal.copy()\n",
    "\n",
    "df['GAP_diff'] = df['GAP_corrected_w2'] - df['GAP_corrected_w1']\n",
    "Q1 = df['GAP_diff'].quantile(0.25)\n",
    "Q3 = df['GAP_diff'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "df['z_score'] = stats.zscore(df['GAP_diff'])\n",
    "\n",
    "outliers = df[(df['GAP_diff'] < lower_bound) | (df['GAP_diff'] > upper_bound)]\n",
    "outliers2 = df[(df['z_score'] > 3) | (df['z_score'] < -3)]\n",
    "\n",
    "\n",
    "df = df[(df['GAP_diff'] >= lower_bound) & (df['GAP_diff'] <= upper_bound)]\n",
    "df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "df['GAP_corrected_w1'] = df['GAP_corrected_w1']/df['delta_time']\n",
    "\n",
    "results_merge_df_all = df.copy()\n",
    "\n",
    "results_merge_df_all= results_merge_df_all[['GAP_corrected_w1', 'Cognition_w2', 'Barthel_w2', 'Well_being_domain_w2']]\n",
    "\n",
    "results_merge_df_all = results_merge_df_all.loc[:, ~results_merge_df_all.columns.duplicated()]\n",
    "\n",
    "\n",
    "results_merge_df_all['Barthel_w2'] = stats.zscore(results_merge_df_all['Barthel_w2'])\n",
    "results_merge_df_all['Cognition_w2'] = stats.zscore(results_merge_df_all['Cognition_w2'])\n",
    "results_merge_df_all['Well_being_domain_w2'] = stats.zscore(results_merge_df_all['Well_being_domain_w2'])\n",
    "\n",
    "\n",
    "\n",
    "results_merge_df_all['GAP_corrected_w1_bin'] = np.where(\n",
    "    results_merge_df_all['GAP_corrected_w1'] > 0, 'Aging',\n",
    "    np.where(\n",
    "        results_merge_df_all['GAP_corrected_w1'] <= 0, 'Healty',\n",
    "        np.nan\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "results_merge_df_all['Cognition_w2_bin'] = np.where(\n",
    "    results_merge_df_all['Cognition_w2'] > results_merge_df_all['Cognition_w2'].median(), 'Good',\n",
    "    np.where(\n",
    "        results_merge_df_all['Cognition_w2'] <= results_merge_df_all['Cognition_w2'].median(), 'Bad',\n",
    "        np.nan\n",
    "    )\n",
    ")\n",
    "\n",
    "results_merge_df_all['Barthel_w2_bin'] = np.where(\n",
    "    results_merge_df_all['Barthel_w2'] > results_merge_df_all['Barthel_w2'].median(), 'Good',\n",
    "    np.where(\n",
    "        results_merge_df_all['Barthel_w2'] <= results_merge_df_all['Barthel_w2'].median(), 'Bad',\n",
    "        np.nan\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "results_merge_df_all['Well_being_domain_w2_bin'] = np.where(\n",
    "    results_merge_df_all['Well_being_domain_w2'] > results_merge_df_all['Well_being_domain_w2'].median(), 'Good',\n",
    "    np.where(\n",
    "        results_merge_df_all['Well_being_domain_w2'] <= results_merge_df_all['Well_being_domain_w2'].median(), 'Bad',\n",
    "        np.nan\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "results_merge_df_all.dropna(inplace =True)\n",
    "results_merge_df_all.reset_index(inplace =True, drop = True)\n",
    "\n",
    "\n",
    "vars_list = ['Cognition_w2', 'Barthel_w2', 'Well_being_domain_w2']\n",
    "\n",
    "\n",
    "df_att = pd.DataFrame(columns=['ci_l', 'ci_r', 'attributable_risk_percent', 'Feature', 'Outcome'])\n",
    "df_rr = pd.DataFrame(columns=['ci_l', 'ci_r', 'realtive_risk', 'Feature', 'Outcome'])\n",
    "\n",
    "count = 0\n",
    "for i in vars_list:\n",
    "    table = create_2x2(results_merge_df_all, 'GAP_corrected_w1_bin', i+'_bin', ['Aging', 'Healty'],  ['Bad', 'Good']) \n",
    "    res = my_attributable_risk(table, False)\n",
    "    attributable_risk_percent = res[0]\n",
    "    ci_l = res[1][0]\n",
    "    ci_r = res[1][1]\n",
    "\n",
    "\n",
    "    print(i+'_bin', results_merge_df_all[i+'_bin'].shape[0])\n",
    "    display(results_merge_df_all[i+'_bin'].value_counts())\n",
    "\n",
    "    table = create_2x2(results_merge_df_all, 'GAP_corrected_w1_bin', i+'_bin', ['Aging', 'Healty'],  [ 'Bad', 'Good']) ## De esta forma los protectores quedan a la izquierda\n",
    "    res = my_attributable_risk(table, False)\n",
    "    \n",
    "    rel_risk = res[2]\n",
    "    ci_lrr = res[3][0]\n",
    "    ci_rrr = res[3][1]\n",
    "\n",
    "    df_att.loc[count, :] = [ ci_l, ci_r, attributable_risk_percent, 'GAP_corrected_w1_bin',i]\n",
    "    df_rr.loc[count, :] = [ ci_lrr, ci_rrr, rel_risk, 'GAP_corrected_w1_bin',i]\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbc0d5-8066-4463-a134-a0ae06fa8b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39edb3f-3871-42bc-8d06-af5183579f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_macro_long_ = pd.read_excel('data/macro_long.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b973daa5-02be-4e3d-877f-1514dbfaf7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_merge_df_all = BAGs_data_longitudinal.copy()\n",
    "\n",
    "results_merge_df_all['GAP_corrected_w1'] = results_merge_df_all['GAP_corrected_w1']/results_merge_df_all['delta_time']\n",
    "results_merge_df_all= results_merge_df_all[['GAP_corrected_w1', 'Cognition_w2', 'Barthel_w2', 'Well_being_domain_w2', 'Region', 'country']]\n",
    "\n",
    "results_merge_df_all = results_merge_df_all.loc[:, ~results_merge_df_all.columns.duplicated()]\n",
    "\n",
    "results_merge_df_all.dropna(inplace =True)\n",
    "results_merge_df_all.reset_index(inplace =True, drop = True)\n",
    "\n",
    "\n",
    "vars_list = ['Cognition_w2', 'Barthel_w2', 'Well_being_domain_w2']\n",
    "\n",
    "\n",
    "count = 0\n",
    "for i in vars_list:\n",
    "\n",
    "    df_rr = pd.DataFrame(columns=['ci_l', 'ci_r', 'rr_SE','realtive_risk', 'Feature', 'Region', 'Country', 'unexposed', 'exposed', 'total'])\n",
    "\n",
    "    for c in results_merge_df_all.country.unique():\n",
    "\n",
    "\n",
    "        results_merge_df_c = results_merge_df_all[results_merge_df_all.country == c]\n",
    "\n",
    "        results_merge_df_c = results_merge_df_c.copy()\n",
    "        \n",
    "        results_merge_df_c['Barthel_w2'] = stats.zscore(results_merge_df_c['Barthel_w2'])\n",
    "        results_merge_df_c['Cognition_w2'] = stats.zscore(results_merge_df_c['Cognition_w2'])\n",
    "        results_merge_df_c['Well_being_domain_w2'] = stats.zscore(results_merge_df_c['Well_being_domain_w2'])\n",
    "        \n",
    "        \n",
    "        results_merge_df_c['GAP_corrected_w1_bin'] = np.where(\n",
    "            results_merge_df_c['GAP_corrected_w1'] > 0, 'Aging',\n",
    "            np.where(\n",
    "                results_merge_df_c['GAP_corrected_w1'] <= 0, 'Healty',\n",
    "                np.nan\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        results_merge_df_c['Cognition_w2_bin'] = np.where(\n",
    "            results_merge_df_c['Cognition_w2'] > np.median(results_merge_df_c.Cognition_w2.unique()), 'Good',\n",
    "            np.where(\n",
    "                results_merge_df_c['Cognition_w2'] <= np.median(results_merge_df_c.Cognition_w2.unique()), 'Bad',\n",
    "                np.nan\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        results_merge_df_c['Barthel_w2_bin'] = np.where(\n",
    "            results_merge_df_c['Barthel_w2'] > np.median(results_merge_df_c.Barthel_w2.unique()), 'Good',\n",
    "            np.where(\n",
    "                results_merge_df_c['Barthel_w2'] <= np.median(results_merge_df_c.Barthel_w2.unique()), 'Bad',\n",
    "                np.nan\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        results_merge_df_c['Well_being_domain_w2_bin'] = np.where(\n",
    "            results_merge_df_c['Well_being_domain_w2'] > np.median(results_merge_df_c.Well_being_domain_w2.unique()), 'Good',\n",
    "            np.where(\n",
    "                results_merge_df_c['Well_being_domain_w2'] <= np.median(results_merge_df_c.Well_being_domain_w2.unique()), 'Bad',\n",
    "                np.nan\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        table = create_2x2(results_merge_df_c, 'GAP_corrected_w1_bin', i+'_bin', ['Aging', 'Healty'],  ['Bad', 'Good'])\n",
    "        res = my_attributable_risk(table, False)\n",
    "        attributable_risk_percent = res[0]\n",
    "        ci_l = res[1][0]\n",
    "        ci_r = res[1][1]\n",
    "    \n",
    "        rel_risk = np.round(res[2],2)\n",
    "        ci_lrr = res[3][0]\n",
    "        ci_rrr = res[3][1]\n",
    "        rr_se =  res[4]\n",
    "\n",
    "        unexposed = dict(Counter(results_merge_df_c[i + '_bin']))['Good']\n",
    "        exposed = dict(Counter(results_merge_df_c[i + '_bin']))['Bad']\n",
    "        total = results_merge_df_c[i + '_bin'].shape[0]\n",
    "\n",
    "    \n",
    "        df_rr.loc[count, :] = [ ci_lrr, ci_rrr, rr_se, rel_risk, i, results_merge_df_c.Region.unique()[0], c, \n",
    "                              unexposed, exposed, total]\n",
    "        \n",
    "        count+=1\n",
    "\n",
    "    df_rr = pd.merge(df_macro_long_, df_rr, left_on=['Country'], right_on=['Country'], how='left')\n",
    "\n",
    "\n",
    "    grouped_avg_rr = df_rr.groupby('Group_GDP')['realtive_risk'].mean()\n",
    "\n",
    "    display(grouped_avg_rr)\n",
    "\n",
    "    display(df_rr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
